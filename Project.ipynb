{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP 2021 Final Project\n",
    "\n",
    "Name 1: Lukas Wilde<br/> \n",
    "Student id 1: 2564597<br/>\n",
    "Email 1: s8luwild@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: Dennis He√ü<br/>\n",
    "Student id 2: 2574005<br/>\n",
    "Email 2: s8dehess@stud.uni-saarland.de<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukas/dev/snlp_project\n"
     ]
    }
   ],
   "source": [
    "%cd ~/dev/snlp_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split: 0.799948 0.200052\n",
      "train-test-split: 0.799757 0.200243\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "PATH = \"./data\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "for name in ['alice_in_wonderland.txt', 'bengali_corpus.txt']:\n",
    "    file = os.path.join(PATH, name)\n",
    "    with open(file, 'r') as f:\n",
    "        \n",
    "        # lowercase input\n",
    "        x = f.read().lower().replace(\"'\", \"\")\n",
    "        \n",
    "        total_symbols = len(x)\n",
    "        \n",
    "        # tokenize sentences\n",
    "        sentences = sent_tokenize(x)\n",
    "        \n",
    "        # remove additional whitespaces\n",
    "        sentences = list(map(lambda x: \" \".join(x.split()), sentences))\n",
    "        \n",
    "        # remove punctuation (only in english corpus, in bengali words are strangely split up by this)\n",
    "        if name == 'alice_in_wonderland.txt':\n",
    "            temp = []\n",
    "            for sent in sentences:\n",
    "                words = word_tokenize(sent)\n",
    "                modified_words = tokenizer.tokenize(' '.join(words))\n",
    "                temp.append(' '.join(modified_words))\n",
    "\n",
    "            sentences = temp\n",
    "        \n",
    "        # remove stopwords\n",
    "        temp = []\n",
    "        for sent in sentences:\n",
    "            temp.append([])\n",
    "            for word in sent.split():\n",
    "                if word not in (stopwords.words('english')):\n",
    "                    temp[-1].append(word)\n",
    "        \n",
    "        sentences = list(map(lambda x: \" \".join(x), temp))\n",
    "        \n",
    "        num_sentences = len(sentences)\n",
    "        train_set = sentences\n",
    "        \n",
    "        # shuffle sentences\n",
    "        random.shuffle(train_set)\n",
    "        \n",
    "        test_set = []\n",
    "        test_symbols = 0\n",
    "        \n",
    "        # get as many sentences in train set, until there are only 80 % of symbols in this set\n",
    "        while test_symbols < 0.2 * total_symbols:\n",
    "            idx = random.randint(0, len(train_set)-1)\n",
    "            test_sentence = train_set.pop(idx)\n",
    "            test_symbols += len(test_sentence)\n",
    "            test_set.append(test_sentence)\n",
    "                \n",
    "        print(\"train-test-split: %f %f\" % (1 - test_symbols/total_symbols, test_symbols/total_symbols))\n",
    "        \n",
    "        # write contents to file\n",
    "        for output in ['train.txt', 'test.txt']:\n",
    "            output_file = os.path.join(PATH, (\"en_\" if name == 'alice_in_wonderland.txt' else 'bn_') + output)\n",
    "            with open(output_file, 'w') as out:\n",
    "                out.write(\"\\n\".join(train_set) if output == 'train.txt' else \"\\n\".join(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/en_train.txt', 'r') as f:\n",
    "    text = f.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/en_test.txt', 'r') as f:\n",
    "    test = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on character level\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_chars --vocab_size=33 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_chars.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('en_s1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('en_test1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on smaller sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_smaller --vocab_size=250 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_smaller.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open(f'en_s2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "\n",
    "with open(f'en_test2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on larger sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_larger --vocab_size=1500 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_larger.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open(f'en_s3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "\n",
    "with open(f'en_test3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bn_train.txt', 'r') as f:\n",
    "    text = f.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/bn_test.txt', 'r') as f:\n",
    "    test = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on character level\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_chars --vocab_size=71 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_chars.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('bn_s1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('bn_test1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on smaller sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_smaller --vocab_size=250 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_smaller.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open(f'bn_s2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "\n",
    "with open(f'bn_test2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on larger sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_larger --vocab_size=1500 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_larger.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('bn_s3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('bn_test3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Ex. 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukas/dev/snlp_project/models\n"
     ]
    }
   ],
   "source": [
    "%cd models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug mode: 2\n",
      "train file: ../bn_s3.txt\n",
      "valid file: ../bn_test3.txt\n",
      "class size: 1\n",
      "Hidden layer size: 300\n",
      "BPTT: 10\n",
      "Rand seed: 1\n",
      "rnnlm file: model_1972_300_10_1\n",
      "Starting training using file ../bn_s3.txt\n",
      "Vocab size: 1972\n",
      "Words in train file: 396857\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 9.3581    Words/sec: 1806.1   VALID entropy: 8.8129\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 8.2916    Words/sec: 1815.7   VALID entropy: 8.1537\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 7.8143    Words/sec: 1814.7   VALID entropy: 7.9017\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 7.5736    Words/sec: 1808.4   VALID entropy: 7.7782\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 7.4128    Words/sec: 1802.8   VALID entropy: 7.7054\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 7.2892    Words/sec: 1814.2   VALID entropy: 7.6533\n",
      "Iter:   6\tAlpha: 0.100000\t   TRAIN entropy: 7.1864    Words/sec: 1814.3   VALID entropy: 7.6174\n",
      "Iter:   7\tAlpha: 0.100000\t   TRAIN entropy: 7.0964    Words/sec: 1814.5   VALID entropy: 7.5942\n",
      "Iter:   8\tAlpha: 0.100000\t   TRAIN entropy: 7.0160    Words/sec: 1764.5   VALID entropy: 7.5905\n",
      "Iter:   9\tAlpha: 0.050000\t   TRAIN entropy: 6.7996    Words/sec: 1777.5   VALID entropy: 7.4749\n",
      "Iter:  10\tAlpha: 0.025000\t   TRAIN entropy: 6.6601    Words/sec: 1812.2   VALID entropy: 7.4065\n",
      "Iter:  11\tAlpha: 0.012500\t   TRAIN entropy: 6.5769    Words/sec: 1801.7   VALID entropy: 7.3668\n",
      "Iter:  12\tAlpha: 0.006250\t   TRAIN entropy: 6.5293    Words/sec: 1806.6   VALID entropy: 7.3436\n",
      "Iter:  13\tAlpha: 0.003125\t   TRAIN entropy: 6.5037    Words/sec: 1775.5   VALID entropy: 7.3282\n"
     ]
    }
   ],
   "source": [
    "# change train_file, test_file to ../en_sx.txt and /en_testx.txt or ../bn_sx.txt and ../bn_testx.txt\n",
    "\n",
    "train_file, test_file = '../bn_s3.txt', '../bn_test3.txt'\n",
    "\n",
    "vocab, hidden, bptt, _class = 1972, 300, 10, 1\n",
    "\n",
    "command = f\"\"\"../rnnlm/rnnlm \\\n",
    "-train {train_file} \\\n",
    "-valid {test_file} \\\n",
    "-rnnlm model_{vocab}_{hidden}_{bptt}_{_class} \\\n",
    "-hidden {hidden} \\\n",
    "-rand-seed 1 \\\n",
    "-debug 2 \\\n",
    "-bptt {bptt} \\\n",
    "-class {_class}\"\"\"\n",
    "\n",
    "with open('rnnlm.sh', 'w') as f:\n",
    "    f.write(command)\n",
    "    \n",
    "!bash rnnlm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Ex. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnlm = 'en_s'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
