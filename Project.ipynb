{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP 2021 Final Project\n",
    "\n",
    "Name 1: Lukas Wilde<br/> \n",
    "Student id 1: 2564597<br/>\n",
    "Email 1: s8luwild@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: Dennis He√ü<br/>\n",
    "Student id 2: 2574005<br/>\n",
    "Email 2: s8dehess@stud.uni-saarland.de<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukas/dev/snlp_project\n"
     ]
    }
   ],
   "source": [
    "%cd ~/dev/snlp_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split: 0.799948 0.200052\n",
      "train-test-split: 0.799757 0.200243\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "PATH = \"./data\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "for name in ['alice_in_wonderland.txt', 'bengali_corpus.txt']:\n",
    "    file = os.path.join(PATH, name)\n",
    "    with open(file, 'r') as f:\n",
    "        \n",
    "        # lowercase input\n",
    "        x = f.read().lower().replace(\"'\", \"\")\n",
    "        \n",
    "        total_symbols = len(x)\n",
    "        \n",
    "        # tokenize sentences\n",
    "        sentences = sent_tokenize(x)\n",
    "        \n",
    "        # remove additional whitespaces\n",
    "        sentences = list(map(lambda x: \" \".join(x.split()), sentences))\n",
    "        \n",
    "        # remove punctuation (only in english corpus, in bengali words are strangely split up by this)\n",
    "        if name == 'alice_in_wonderland.txt':\n",
    "            temp = []\n",
    "            for sent in sentences:\n",
    "                words = word_tokenize(sent)\n",
    "                modified_words = tokenizer.tokenize(' '.join(words))\n",
    "                temp.append(' '.join(modified_words))\n",
    "\n",
    "            sentences = temp\n",
    "        \n",
    "        # remove stopwords\n",
    "        temp = []\n",
    "        for sent in sentences:\n",
    "            temp.append([])\n",
    "            for word in sent.split():\n",
    "                if word not in (stopwords.words('english')):\n",
    "                    temp[-1].append(word)\n",
    "        \n",
    "        sentences = list(map(lambda x: \" \".join(x), temp))\n",
    "        \n",
    "        num_sentences = len(sentences)\n",
    "        train_set = sentences\n",
    "        \n",
    "        # shuffle sentences\n",
    "        random.shuffle(train_set)\n",
    "        \n",
    "        test_set = []\n",
    "        test_symbols = 0\n",
    "        \n",
    "        # get as many sentences in train set, until there are only 80 % of symbols in this set\n",
    "        while test_symbols < 0.2 * total_symbols:\n",
    "            idx = random.randint(0, len(train_set)-1)\n",
    "            test_sentence = train_set.pop(idx)\n",
    "            test_symbols += len(test_sentence)\n",
    "            test_set.append(test_sentence)\n",
    "                \n",
    "        print(\"train-test-split: %f %f\" % (1 - test_symbols/total_symbols, test_symbols/total_symbols))\n",
    "        \n",
    "        # write contents to file\n",
    "        for output in ['train.txt', 'test.txt']:\n",
    "            output_file = os.path.join(PATH, (\"en_\" if name == 'alice_in_wonderland.txt' else 'bn_') + output)\n",
    "            with open(output_file, 'w') as out:\n",
    "                out.write(\"\\n\".join(train_set) if output == 'train.txt' else \"\\n\".join(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/en_train.txt', 'r') as f:\n",
    "    text = f.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/en_test.txt', 'r') as f:\n",
    "    test = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on character level\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_chars --vocab_size=33 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_chars.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('en_s1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('en_test1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on smaller sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_smaller --vocab_size=250 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_smaller.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open(f'en_s2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "\n",
    "with open(f'en_test2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on larger sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_larger --vocab_size=1500 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_larger.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open(f'en_s3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "\n",
    "with open(f'en_test3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bn_train.txt', 'r') as f:\n",
    "    text = f.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/bn_test.txt', 'r') as f:\n",
    "    test = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on character level\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_chars --vocab_size=71 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_chars.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('bn_s1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('bn_test1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on smaller sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_smaller --vocab_size=200 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_smaller.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open(f'bn_s2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "\n",
    "with open(f'bn_test2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on larger sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_larger --vocab_size=1500 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_larger.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('bn_s3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('bn_test3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Ex. 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukas/dev/snlp_project/models\n"
     ]
    }
   ],
   "source": [
    "%cd models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug mode: 2\n",
      "train file: ../bn_s1.txt\n",
      "valid file: ../bn_test1.txt\n",
      "class size: 544\n",
      "Min improvement: 1.000000\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: model_544_40_3_544\n",
      "Starting training using file ../bn_s1.txt\n",
      "Vocab size: 544\n",
      "Words in train file: 1209947\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 3.2364    Words/sec: 44748.2   VALID entropy: 3.1746\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 3.0909    Words/sec: 44721.5   VALID entropy: 3.1526\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 3.0685    Words/sec: 44711.9   VALID entropy: 3.1391\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 3.0583    Words/sec: 44778.6   VALID entropy: 3.1236\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 3.0503    Words/sec: 44701.6   VALID entropy: 3.1293\n",
      "Iter:   5\tAlpha: 0.050000\t   TRAIN entropy: 2.9996    Words/sec: 44631.8   VALID entropy: 3.0401\n",
      "Iter:   6\tAlpha: 0.025000\t   TRAIN entropy: 2.9697    Words/sec: 43922.7   VALID entropy: 2.9922\n",
      "Iter:   7\tAlpha: 0.012500\t   TRAIN entropy: 2.9549    Words/sec: 45505.5   VALID entropy: 2.9643\n",
      "Iter:   8\tAlpha: 0.006250\t   TRAIN entropy: 2.9477    Words/sec: 45452.7   VALID entropy: 2.9509\n",
      "Iter:   9\tAlpha: 0.003125\t   TRAIN entropy: 2.9441    Words/sec: 45437.4   VALID entropy: 2.9439\n",
      "Iter:  10\tAlpha: 0.001563\t   TRAIN entropy: 2.9423    Words/sec: 45292.6   VALID entropy: 2.9397\n",
      "Iter:  11\tAlpha: 0.000781\t   TRAIN entropy: 2.9412    Words/sec: 45378.8   VALID entropy: 2.9372\n",
      "Iter:  12\tAlpha: 0.000391\t   TRAIN entropy: 2.9406    Words/sec: 45351.6   VALID entropy: 2.9359\n",
      "Iter:  13\tAlpha: 0.000195\t   TRAIN entropy: 2.9403    Words/sec: 45422.5   VALID entropy: 2.9352\n",
      "Iter:  14\tAlpha: 0.000098\t   TRAIN entropy: 2.9400    Words/sec: 45359.7   VALID entropy: 2.9349\n",
      "Iter:  15\tAlpha: 0.000049\t   TRAIN entropy: 2.9399    Words/sec: 45423.3   VALID entropy: 2.9347\n",
      "Iter:  16\tAlpha: 0.000024\t   TRAIN entropy: 2.9398    Words/sec: 45338.1   VALID entropy: 2.9346\n",
      "Iter:  17\tAlpha: 0.000012\t   TRAIN entropy: 2.9398    Words/sec: 45469.2   VALID entropy: 2.9346\n",
      "Iter:  18\tAlpha: 0.000006\t   TRAIN entropy: 2.9398    Words/sec: 45365.3   VALID entropy: 2.9346\n",
      "Iter:  19\tAlpha: 0.000003\t   TRAIN entropy: 2.9397    Words/sec: 45388.3   VALID entropy: 2.9345\n",
      "Iter:  20\tAlpha: 0.000002\t   TRAIN entropy: 2.9397    Words/sec: 45325.5   VALID entropy: 2.9345\n",
      "Iter:  21\tAlpha: 0.000001\t   TRAIN entropy: 2.9397    Words/sec: 45375.5   VALID entropy: 2.9345\n",
      "Iter:  22\tAlpha: 0.000000\t   TRAIN entropy: 2.9397    Words/sec: 45436.5   VALID entropy: 2.9345\n",
      "Iter:  23\tAlpha: 0.000000\t   TRAIN entropy: 2.9343    Progress: 8.26%   Words/sec: 44929.5 ^C\n"
     ]
    }
   ],
   "source": [
    "# change train_file, test_file to ../en_sx.txt and /en_testx.txt or ../bn_sx.txt and ../bn_testx.txt\n",
    "\n",
    "train_file, test_file = '../bn_s1.txt', '../bn_test1.txt'\n",
    "\n",
    "vocab, hidden, bptt, _class = 544, 40, 3, 544\n",
    "\n",
    "command = f\"\"\"../rnnlm/rnnlm \\\n",
    "-train {train_file} \\\n",
    "-valid {test_file} \\\n",
    "-rnnlm model_{vocab}_{hidden}_{bptt}_{_class} \\\n",
    "-hidden {hidden} \\\n",
    "-rand-seed 1 \\\n",
    "-debug 2 \\\n",
    "-bptt {bptt} \\\n",
    "-class {_class}\"\"\"\n",
    "\n",
    "with open('rnnlm.sh', 'w') as f:\n",
    "    f.write(command)\n",
    "    \n",
    "!bash rnnlm.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
