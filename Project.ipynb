{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP 2021 Final Project\n",
    "\n",
    "Name 1: Lukas Wilde<br/> \n",
    "Student id 1: 2564597<br/>\n",
    "Email 1: s8luwild@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: Dennis Heß<br/>\n",
    "Student id 2: 2574005<br/>\n",
    "Email 2: s8dehess@stud.uni-saarland.de<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lukas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lukas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukas/dev/snlp_project\n"
     ]
    }
   ],
   "source": [
    "%cd ~/dev/snlp_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/stopwords-iso/stopwords-bn\n",
    "\n",
    "bengali_stopwords = [\n",
    "    \"অতএব\", \"অথচ\", \"অথবা\", \"অনুযায়ী\", \"অনেক\", \"অনেকে\", \"অনেকেই\", \"অন্তত\", \"অন্য\", \"অবধি\", \"অবশ্য\", \n",
    "    \"অর্থাত\", \"আই\", \"আগামী\", \"আগে\", \"আগেই\", \"আছে\", \"আজ\", \"আদ্যভাগে\", \"আপনার\", \"আপনি\", \"আবার\", \"আমরা\", \n",
    "    \"আমাকে\", \"আমাদের\", \"আমার\", \"আমি\", \"আর\", \"আরও\", \"ই\", \"ইত্যাদি\", \"ইহা\", \"উচিত\", \"উত্তর\", \"উনি\", \"উপর\", \n",
    "    \"উপরে\", \"এ\", \"এঁদের\", \"এঁরা\", \"এই\", \"একই\", \"একটি\", \"একবার\", \"একে\", \"এক্\", \"এখন\", \"এখনও\", \"এখানে\", \n",
    "    \"এখানেই\", \"এটা\", \"এটাই\", \"এটি\", \"এত\", \"এতটাই\", \"এতে\", \"এদের\", \"এব\", \"এবং\", \"এবার\", \"এমন\", \"এমনকী\", \n",
    "    \"এমনি\", \"এর\", \"এরা\", \"এল\", \"এস\", \"এসে\", \"ঐ\", \"ও\", \"ওঁদের\", \"ওঁর\", \"ওঁরা\", \"ওই\", \"ওকে\", \"ওখানে\", \n",
    "    \"ওদের\", \"ওর\", \"ওরা\", \"কখনও\", \"কত\", \"কবে\", \"কমনে\", \"কয়েক\", \"কয়েকটি\", \"করছে\", \"করছেন\", \"করতে\", \"করবে\", \n",
    "    \"করবেন\", \"করলে\", \"করলেন\", \"করা\", \"করাই\", \"করায়\", \"করার\", \"করি\", \"করিতে\", \"করিয়া\", \"করিয়ে\", \"করে\", \"করেই\", \n",
    "    \"করেছিলেন\", \"করেছে\", \"করেছেন\", \"করেন\", \"কাউকে\", \"কাছ\", \"কাছে\", \"কাজ\", \"কাজে\", \"কারও\", \"কারণ\", \"কি\", \"কিংবা\", \n",
    "    \"কিছু\", \"কিছুই\", \"কিন্তু\", \"কী\", \"কে\", \"কেউ\", \"কেউই\", \"কেখা\", \"কেন\", \"কোটি\", \"কোন\", \"কোনও\", \"কোনো\", \n",
    "    \"ক্ষেত্রে\", \"কয়েক\", \"খুব\", \"গিয়ে\", \"গিয়েছে\", \"গিয়ে\", \"গুলি\", \"গেছে\", \"গেল\", \"গেলে\", \"গোটা\", \"চলে\", \"চান\", \n",
    "    \"চায়\", \"চার\", \"চালু\", \"চেয়ে\", \"চেষ্টা\", \"ছাড়া\", \"ছাড়াও\", \"ছিল\", \"ছিলেন\", \"জন\", \"জনকে\", \"জনের\", \"জন্য\", \"জন্যওজে\", \n",
    "    \"জানতে\", \"জানা\", \"জানানো\", \"জানায়\", \"জানিয়ে\", \"জানিয়েছে\", \"জে\", \"জ্নজন\", \"টি\", \"ঠিক\", \"তখন\", \"তত\", \"তথা\", \"তবু\", \n",
    "    \"তবে\", \"তা\", \"তাঁকে\", \"তাঁদের\", \"তাঁর\", \"তাঁরা\", \"তাঁাহারা\", \"তাই\", \"তাও\", \"তাকে\", \"তাতে\", \"তাদের\", \"তার\", \n",
    "    \"তারপর\", \"তারা\", \"তারৈ\", \"তাহলে\", \"তাহা\", \"তাহাতে\", \"তাহার\", \"তিনঐ\", \"তিনি\", \"তিনিও\", \"তুমি\", \"তুলে\", \"তেমন\", \n",
    "    \"তো\", \"তোমার\", \"থাকবে\", \"থাকবেন\", \"থাকা\", \"থাকায়\", \"থাকে\", \"থাকেন\", \"থেকে\", \"থেকেই\", \"থেকেও\", \"দিকে\", \"দিতে\", \n",
    "    \"দিন\", \"দিয়ে\", \"দিয়েছে\", \"দিয়েছেন\", \"দিলেন\", \"দু\", \"দুই\", \"দুটি\", \"দুটো\", \"দেওয়া\", \"দেওয়ার\", \"দেওয়া\", \"দেখতে\", \n",
    "    \"দেখা\", \"দেখে\", \"দেন\", \"দেয়\", \"দ্বারা\", \"ধরা\", \"ধরে\", \"ধামার\", \"নতুন\", \"নয়\", \"না\", \"নাই\", \"নাকি\", \"নাগাদ\", \"নানা\", \n",
    "    \"নিজে\", \"নিজেই\", \"নিজেদের\", \"নিজের\", \"নিতে\", \"নিয়ে\", \"নিয়ে\", \"নেই\", \"নেওয়া\", \"নেওয়ার\", \"নেওয়া\", \"নয়\", \"পক্ষে\", \n",
    "    \"পর\", \"পরে\", \"পরেই\", \"পরেও\", \"পর্যন্ত\", \"পাওয়া\", \"পাচ\", \"পারি\", \"পারে\", \"পারেন\", \"পি\", \"পেয়ে\", \"পেয়্র্\", \"প্রতি\", \n",
    "    \"প্রথম\", \"প্রভৃতি\", \"প্রযন্ত\", \"প্রাথমিক\", \"প্রায়\", \"প্রায়\", \"ফলে\", \"ফিরে\", \"ফের\", \"বক্তব্য\", \"বদলে\", \"বন\", \"বরং\", \"বলতে\", \n",
    "    \"বলল\", \"বললেন\", \"বলা\", \"বলে\", \"বলেছেন\", \"বলেন\", \"বসে\", \"বহু\", \"বা\", \"বাদে\", \"বার\", \"বি\", \"বিনা\", \"বিভিন্ন\", \n",
    "    \"বিশেষ\", \"বিষয়টি\", \"বেশ\", \"বেশি\", \"ব্যবহার\", \"ব্যাপারে\", \"ভাবে\", \"ভাবেই\", \"মতো\", \"মতোই\", \"মধ্যভাগে\", \"মধ্যে\", \n",
    "    \"মধ্যেই\", \"মধ্যেও\", \"মনে\", \"মাত্র\", \"মাধ্যমে\", \"মোট\", \"মোটেই\", \"যখন\", \"যত\", \"যতটা\", \"যথেষ্ট\", \"যদি\", \"যদিও\", \n",
    "    \"যা\", \"যাঁর\", \"যাঁরা\", \"যাওয়া\", \"যাওয়ার\", \"যাওয়া\", \"যাকে\", \"যাচ্ছে\", \"যাতে\", \"যাদের\", \"যান\", \"যাবে\", \"যায়\", \n",
    "    \"যার\", \"যারা\", \"যিনি\", \"যে\", \"যেখানে\", \"যেতে\", \"যেন\", \"যেমন\", \"র\", \"রকম\", \"রয়েছে\", \"রাখা\", \"রেখে\", \"লক্ষ\", \n",
    "    \"শুধু\", \"শুরু\", \"সঙ্গে\", \"সঙ্গেও\", \"সব\", \"সবার\", \"সমস্ত\", \"সম্প্রতি\", \"সহ\", \"সহিত\", \"সাধারণ\", \"সামনে\", \"সি\", \n",
    "    \"সুতরাং\", \"সে\", \"সেই\", \"সেখান\", \"সেখানে\", \"সেটা\", \"সেটাই\", \"সেটাও\", \"সেটি\", \"স্পষ্ট\", \"স্বয়ং\", \"হইতে\", \n",
    "    \"হইবে\", \"হইয়া\", \"হওয়া\", \"হওয়ায়\", \"হওয়ার\", \"হচ্ছে\", \"হত\", \"হতে\", \"হতেই\", \"হন\", \"হবে\", \"হবেন\", \"হয়\", \n",
    "    \"হয়তো\", \"হয়নি\", \"হয়ে\", \"হয়েই\", \"হয়েছিল\", \"হয়েছে\", \"হয়েছেন\", \"হল\", \"হলে\", \"হলেই\", \"হলেও\", \"হলো\", \n",
    "    \"হাজার\", \"হিসাবে\", \"হৈলে\", \"হোক\", \"হয়\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split: 0.799879 0.200121\n",
      "train-test-split: 0.782961 0.217039\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "PATH = \"./data\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "for name in ['alice_in_wonderland.txt', 'bengali_corpus.txt']:\n",
    "    file = os.path.join(PATH, name)\n",
    "    with open(file, 'r') as f:\n",
    "        \n",
    "        # lowercase input\n",
    "        x = f.read().lower()\n",
    "        \n",
    "        total_symbols = len(x)\n",
    "        \n",
    "        # tokenize sentences\n",
    "        sentences = sent_tokenize(x)\n",
    "        \n",
    "        # remove additional whitespaces\n",
    "        sentences = list(map(lambda x: \" \".join(x.split()), sentences))\n",
    "        \n",
    "        # remove punctuation\n",
    "        temp = []\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent)\n",
    "            modified_words = tokenizer.tokenize(' '.join(words))\n",
    "            temp.append(' '.join(modified_words))\n",
    "            \n",
    "        sentences = temp\n",
    "        \n",
    "        # remove stopwords\n",
    "        temp = []\n",
    "        for sent in sentences:\n",
    "            temp.append([])\n",
    "            for word in sent.split():\n",
    "                if word not in (stopwords.words('english') + bengali_stopwords):\n",
    "                    temp[-1].append(word)\n",
    "        \n",
    "        sentences = list(map(lambda x: \" \".join(x), temp))\n",
    "        \n",
    "        num_sentences = len(sentences)\n",
    "        train_set = sentences\n",
    "        \n",
    "        # shuffle sentences\n",
    "        random.shuffle(train_set)\n",
    "        \n",
    "        test_set = []\n",
    "        test_symbols = 0\n",
    "        \n",
    "        # get as many sentences in train set, until there are only 80 % of symbols in this set\n",
    "        while test_symbols < 0.2 * total_symbols:\n",
    "            idx = random.randint(0, len(train_set)-1)\n",
    "            test_sentence = train_set.pop(idx)\n",
    "            test_symbols += len(test_sentence)\n",
    "            test_set.append(test_sentence)\n",
    "                \n",
    "        print(\"train-test-split: %f %f\" % (1 - test_symbols/total_symbols, test_symbols/total_symbols))\n",
    "        \n",
    "        # write contents to file\n",
    "        for output in ['train.txt', 'test.txt']:\n",
    "            output_file = os.path.join(PATH, (\"en_\" if name == 'alice_in_wonderland.txt' else 'bn_') + output)\n",
    "            with open(output_file, 'w') as out:\n",
    "                out.write(\"\\n\".join(train_set) if output == 'train.txt' else \"\\n\".join(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/en_train.txt', 'r') as f:\n",
    "    text = f.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/en_test.txt', 'r') as f:\n",
    "    test = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on character level\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_chars --vocab_size=33 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_chars.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('en_s1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('en_test1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on smaller sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_smaller --vocab_size=400 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_smaller.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('en_s2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('en_test2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on larger sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_larger --vocab_size=2500 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_larger.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('en_s3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('en_test3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bn_train.txt', 'r') as f:\n",
    "    text = f.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/bn_test.txt', 'r') as f:\n",
    "    test = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on character level\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_chars --vocab_size=46 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_chars.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('bn_s1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('bn_test1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on smaller sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_smaller --vocab_size=400 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_smaller.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('bn_s2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('bn_test2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on larger sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_larger --vocab_size=2500 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_larger.model')\n",
    "\n",
    "encoded_train = sp.EncodeAsPieces(text)\n",
    "encoded_test = sp.EncodeAsPieces(test)\n",
    "\n",
    "with open('bn_s3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_train))\n",
    "    \n",
    "with open('bn_test3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Ex. 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukas/dev/snlp_project/models\n"
     ]
    }
   ],
   "source": [
    "%cd models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug mode: 2\n",
      "train file: ../bn_s1.txt\n",
      "valid file: ../bn_test1.txt\n",
      "class size: 1\n",
      "Hidden layer size: 300\n",
      "BPTT: 10\n",
      "Rand seed: 1\n",
      "rnnlm file: model_443_300_10_1\n",
      "Starting training using file ../bn_s1.txt\n",
      "Vocab size: 443\n",
      "Words in train file: 812260\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 2.7136    Words/sec: 4355.7   VALID entropy: 2.6382\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 2.5431    Words/sec: 4371.5   VALID entropy: 2.5551\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 2.4801    Words/sec: 4340.1   VALID entropy: 2.5120\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 2.4423    Words/sec: 4414.7   VALID entropy: 2.4840\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 2.4155    Words/sec: 4231.5   VALID entropy: 2.4673\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 2.3949    Words/sec: 4236.6   VALID entropy: 2.4517\n",
      "Iter:   6\tAlpha: 0.100000\t   TRAIN entropy: 2.3779    Words/sec: 4236.5   VALID entropy: 2.4416\n",
      "Iter:   7\tAlpha: 0.100000\t   TRAIN entropy: 2.3633    Words/sec: 4204.2   VALID entropy: 2.4334\n",
      "Iter:   8\tAlpha: 0.100000\t   TRAIN entropy: 2.3508    Words/sec: 4372.9   VALID entropy: 2.4266\n",
      "Iter:   9\tAlpha: 0.050000\t   TRAIN entropy: 2.3058    Words/sec: 4265.8   VALID entropy: 2.3797\n",
      "Iter:  10\tAlpha: 0.025000\t   TRAIN entropy: 2.2793    Words/sec: 4239.2   VALID entropy: 2.3540\n",
      "Iter:  11\tAlpha: 0.012500\t   TRAIN entropy: 2.2644    Words/sec: 4242.3   VALID entropy: 2.3393\n",
      "Iter:  12\tAlpha: 0.006250\t   TRAIN entropy: 2.2561    Words/sec: 4239.1   VALID entropy: 2.3318\n",
      "Iter:  13\tAlpha: 0.003125\t   TRAIN entropy: 2.2515    Words/sec: 4241.2   VALID entropy: 2.3279\n"
     ]
    }
   ],
   "source": [
    "# change train_file, test_file to ../en_sx.txt and /en_testx.txt or ../bn_sx.txt and ../bn_testx.txt\n",
    "\n",
    "train_file, test_file = '../bn_s1.txt', '../bn_test1.txt'\n",
    "\n",
    "vocab, hidden, bptt, _class = 443, 300, 10, 1\n",
    "\n",
    "command = f\"\"\"../rnnlm/rnnlm \\\n",
    "-train {train_file} \\\n",
    "-valid {test_file} \\\n",
    "-rnnlm model_{vocab}_{hidden}_{bptt}_{_class} \\\n",
    "-hidden {hidden} \\\n",
    "-rand-seed 1 \\\n",
    "-debug 2 \\\n",
    "-bptt {bptt} \\\n",
    "-class {_class}\"\"\"\n",
    "\n",
    "with open('rnnlm.sh', 'w') as f:\n",
    "    f.write(command)\n",
    "    \n",
    "!bash rnnlm.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
