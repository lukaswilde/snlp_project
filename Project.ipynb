{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP 2021 Final Project\n",
    "\n",
    "Name 1: Lukas Wilde<br/> \n",
    "Student id 1: 2564597<br/>\n",
    "Email 1: s8luwild@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: Dennis He√ü<br/>\n",
    "Student id 2: 2574005<br/>\n",
    "Email 2: s8dehess@stud.uni-saarland.de<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from nltk import sent_tokenize\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split: 0.799879 0.200121\n",
      "train-test-split: 0.799472 0.200528\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "PATH = \"./data\"\n",
    "\n",
    "for name in ['alice_in_wonderland.txt', 'bengali_corpus.txt']:\n",
    "    file = os.path.join(PATH, name)\n",
    "    with open(file, 'r') as f:\n",
    "        x = f.read()\n",
    "        total_symbols = len(x)\n",
    "        \n",
    "        sentences = sent_tokenize(x)\n",
    "        sentences = list(map(lambda x: \" \".join(x.split()), sentences))\n",
    "        \n",
    "        num_sentences = len(sentences)\n",
    "        \n",
    "        train_set = sentences\n",
    "        \n",
    "        test_set = []\n",
    "        test_symbols = 0\n",
    "        \n",
    "        while test_symbols < 0.2 * total_symbols:\n",
    "            idx = random.randint(0, len(train_set)-1)\n",
    "            test_sentence = train_set.pop(idx)\n",
    "            test_symbols += len(test_sentence)\n",
    "            test_set.append(test_sentence)\n",
    "                \n",
    "        print(\"train-test-split: %f %f\" % (1 - test_symbols/total_symbols, test_symbols/total_symbols))\n",
    "        \n",
    "        \n",
    "        for output in ['train.txt', 'test.txt']:\n",
    "            output_file = os.path.join(PATH, (\"en_\" if name == 'alice_in_wonderland.txt' else 'bn_') + output)\n",
    "            with open(output_file, 'w') as out:\n",
    "                out.write(\"\\n\".join(train_set) if output == 'train.txt' else \"\\n\".join(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/en_train.txt', 'r') as f:\n",
    "    text = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on character level\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_chars --vocab_size=74 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_chars.model')\n",
    "\n",
    "encoded = sp.EncodeAsPieces(text)\n",
    "\n",
    "with open('en_s1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on smaller sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_smaller --vocab_size=400 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_smaller.model')\n",
    "\n",
    "encoded = sp.EncodeAsPieces(text)\n",
    "\n",
    "with open('en_s2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English on larger sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/en_train.txt --model_prefix=en_larger --vocab_size=2500 --character_coverage=1.0 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('en_larger.model')\n",
    "\n",
    "encoded = sp.EncodeAsPieces(text)\n",
    "\n",
    "with open('en_s3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bn_train.txt', 'r') as f:\n",
    "    text = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on character level\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_chars --vocab_size=186 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_chars.model')\n",
    "\n",
    "encoded = sp.EncodeAsPieces(text)\n",
    "\n",
    "with open('bn_s1.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on smaller sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_smaller --vocab_size=400 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_smaller.model')\n",
    "\n",
    "encoded = sp.EncodeAsPieces(text)\n",
    "\n",
    "with open('bn_s2.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengali on larger sub units\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f'--input=data/bn_train.txt --model_prefix=bn_larger --vocab_size=2500 --character_coverage=0.995 --model_type=bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bn_larger.model')\n",
    "\n",
    "encoded = sp.EncodeAsPieces(text)\n",
    "\n",
    "with open('bn_s3.txt', 'w') as f:\n",
    "    f.write(' '.join(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "snlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
